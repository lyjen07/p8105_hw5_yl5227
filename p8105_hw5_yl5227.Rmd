---
title: "p8105_hw5_yl5227"
author: "Jennifer Li"
date: "2023-11-15"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

Import dataset, create variable.

```{r}
df_homocide = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    status = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved",
      ))
```

This dataset gathered data on homicides in 50 large U.S. cities. The resulting dataframe has `r nrow(df_homocide)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `status` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error.

Let's summarize total number of homicides and unsolved homicides within cities.

```{r}
df_city_homcide =
  df_homocide |> 
  select(city_state, status) |> 
  summarize(
    total_homocide = n(),
    unsolved_homocide = sum(status == "unsolved"))
```


# Problem 2

This zip file contains data that allows tracking of changes in observations over multiple weeks for each subject within their assigned group.

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time.

```{r message = FALSE}
df_all_participants =
  tibble(
    file_names = list.files(path = "data", pattern = "(con|exp)_\\d\\d.csv", full.names = TRUE),
    data = map(file_names, read_csv)) |> 
  separate(file_names, into = c("group", "subject_id"), sep = "\\_") |> 
  mutate(
    group = str_remove(group, "data/"),
    subject_id = str_remove(subject_id, ".csv")
  ) |> 
  unnest(data) |> 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value"
  ) |> 
  mutate(week = as.numeric(week))

df_all_participants
```

The provided dataframe `df_all_participants` contains longitudinal data from a study with both control and experimental groups. It has a total of `r nrow(df_all_participants)` entries, structured on variables that include `group` to specify the arm (control or experimental), `subject_id` for participant identification, `week` to denote the time point of the observation, and `value` representing the measurement or observation recorded. Each participant's data is compiled in a distinct file, named with their respective subject ID and group designation.

Make a spaghetti plot

```{r}
df_all_participants |> 
  ggplot(aes(x = week, y = value, group = str_c(subject_id,group), color = group)) +
  geom_line() +
  labs(title = "Values on each Subject Over Time")
```

In the control group, the data points for each participant remain relatively stable, showing little change over time. However, in the experimental group, there's a clear pattern of growth, with measurements for each subject showing an upward trajectory with each passing week.

# Problem 3

Set a seed for reproducibility

```{r}
set.seed(12345)
```


Write a function to perform a one-sample t-test on 5000 independent datasets from a normal distribution with parameters of `mu` = 0, `sigma` = 5, `n`= 30, and a significance level of 0.05.

```{r}
one_sample_t_test = function(mu = 0, sigma = 5, n = 30, num_sample = 5000){
  dataset = 
    map(.x = rep(mu, num_sample), .f = ~tibble(data=rnorm(n,.x,sigma))) |>
    map(t.test) |>
    map_df(broom::tidy)

  p_rej = dataset |>
    filter(p.value < 0.05) |>
    nrow() %>%
    `/`(num_sample)

  mu_est = dataset |>
    pull(estimate) |>
    mean()
  
  mu_rej = dataset |>
    filter(p.value < 0.05) |>
    pull(estimate) |>
    mean()
  
  tibble(mu = mu, p_rej = p_rej, mu_est = mu_est, mu_rej = mu_rej)
}
```

Iterating the function for `mu` ={0,1,2,3,4,5,6}:

```{r, cache = TRUE}
exp_res = 
  0:6 |>
  map_df(one_sample_t_test)

exp_res
```

Then, make a plot for the percentage of samples where null was rejected p<0.05.

```{r}
exp_res |>
  ggplot(aes(x = mu,y = p_rej)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent_format(scale = 100)) +
  scale_x_continuous(breaks = 0:6) +
  ylab("percentage of rejection") +
  labs(title = "Percentage of Rejection over Different mu")
```

Obviously, with the increase of `mu`, the power of the test increased quickly, and saturated at about `mu`=5. It is also notable that when `mu`= 0, the percentage of rejection is not the power of the test, but the percentage of type-I error, because the null hypothesis is true.

Finally, make a plot showing the comparison of `estimated mu` between all and rejected samples.

```{r}
exp_res |>
  pivot_longer(
    mu_est:mu_rej,
    names_to = "mu_type",
    values_to = "mu_est"
  ) |>
  ggplot(aes(x = mu, y = mu_est, color = mu_type)) +
  geom_line() +
  scale_color_discrete(labels = c("all", "rejected")) +
  scale_x_continuous(breaks = 0:6) +
  ylab("estimated mu") +
  labs(title = "Comparison of estimated mu between all and rejected samples")
```

Because the number of rejected samples is low when `mu`=0, the estimated mu for rejection is quite unstable around 0.

For `mu` = {1,2,3,4,5,6}, `mu_rej` is higher than `mu_all`. However, the gap `mu_rej` - `mu_all` gradually decreases with the increase of `mu`. Because when `mu` is equal or greater than 5, nearly all samples are rejected, `mu_rej` approximately equals to `mu_all` under such conditions.